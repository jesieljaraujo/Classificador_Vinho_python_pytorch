"""
Classificador de Vinhos Australianos usando PyTorch
Este projeto demonstra os conceitos fundamentais de Machine Learning:
- Redes Neurais Artificiais
- Treinamento com Backpropagation
- Avalia√ß√£o de Modelos
- Predi√ß√µes
"""

import torch
import torch.nn as nn  # Neural Network modules
import torch.optim as optim  # Optimization algorithms
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.datasets import load_wine  # Dataset de vinhos
from sklearn.model_selection import train_test_split  # Divis√£o treino/teste
from sklearn.preprocessing import StandardScaler  # Normaliza√ß√£o
import matplotlib.pyplot as plt

# ==============================================================================
# 1. PREPARA√á√ÉO DOS DADOS (Data Preparation)
# ==============================================================================

print("=" * 60)
print("CLASSIFICADOR DE VINHOS AUSTRALIANOS - PyTorch")
print("=" * 60)

# Carregar dataset de vinhos (3 tipos de uva: Shiraz, Chardonnay, Cabernet)
wine_data = load_wine()
X = wine_data.data  # Features: acidez, √°lcool, cor, etc.
y = wine_data.target  # Labels: 0, 1, 2 (tipos de vinho)

print(f"\nüìä Dataset carregado:")
print(f"   - Amostras: {X.shape[0]}")
print(f"   - Features: {X.shape[1]}")
print(f"   - Classes: {len(np.unique(y))}")
print(f"   - Nomes das classes: {wine_data.target_names}")

# Dividir dados em treino (80%) e teste (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,  # 20% para teste
    random_state=42,  # Seed para reprodutibilidade
    stratify=y  # Manter propor√ß√£o das classes
)

# Normalizar os dados (m√©dia=0, desvio padr√£o=1)
# Isso ajuda o modelo a convergir mais r√°pido
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # Fit no treino
X_test = scaler.transform(X_test)  # Apenas transform no teste

# Converter para tensores PyTorch (estrutura de dados do PyTorch)
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.LongTensor(y_train)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.LongTensor(y_test)

print(f"\n‚úì Dados normalizados e convertidos para tensores PyTorch")

# ==============================================================================
# 2. CRIA√á√ÉO DO DATASET CUSTOMIZADO (Custom Dataset)
# ==============================================================================

class WineDataset(Dataset):
    """
    Dataset customizado para carregar dados de vinho
    PyTorch usa essa classe para gerenciar dados durante o treinamento
    """
    def __init__(self, X, y):
        # Construtor: inicializa o dataset
        self.X = X
        self.y = y
    
    def __len__(self):
        # Retorna o tamanho do dataset
        return len(self.X)
    
    def __getitem__(self, idx):
        # Retorna um item (amostra) pelo √≠ndice
        return self.X[idx], self.y[idx]

# Criar datasets
train_dataset = WineDataset(X_train_tensor, y_train_tensor)
test_dataset = WineDataset(X_test_tensor, y_test_tensor)

# DataLoader: carrega dados em lotes (batches) durante o treinamento
train_loader = DataLoader(
    train_dataset, 
    batch_size=16,  # Processa 16 amostras por vez
    shuffle=True  # Embaralha os dados a cada √©poca
)

test_loader = DataLoader(
    test_dataset, 
    batch_size=16, 
    shuffle=False  # N√£o embaralha os dados de teste
)

print(f"‚úì DataLoaders criados (batch_size=16)")

# ==============================================================================
# 3. DEFINI√á√ÉO DO MODELO (Neural Network Architecture)
# ==============================================================================

class WineClassifier(nn.Module):
    """
    Rede Neural Feedforward para classifica√ß√£o de vinhos
    Arquitetura: Input -> Hidden Layer 1 -> Hidden Layer 2 -> Output
    """
    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):
        # Chamar construtor da classe pai
        super(WineClassifier, self).__init__()
        
        # Camada 1: Input -> Hidden Layer 1
        self.fc1 = nn.Linear(input_size, hidden_size1)
        
        # Camada 2: Hidden Layer 1 -> Hidden Layer 2
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        
        # Camada 3: Hidden Layer 2 -> Output
        self.fc3 = nn.Linear(hidden_size2, num_classes)
        
        # Fun√ß√£o de ativa√ß√£o ReLU (Rectified Linear Unit)
        # ReLU(x) = max(0, x) - Introduz n√£o-linearidade
        self.relu = nn.ReLU()
        
        # Dropout: desliga aleatoriamente 20% dos neur√¥nios durante treino
        # Isso previne overfitting (memoriza√ß√£o dos dados)
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        """
        Forward pass: define como os dados fluem pela rede
        """
        # Passa pela primeira camada e aplica ReLU
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # Passa pela segunda camada e aplica ReLU
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # Camada de sa√≠da (sem ativa√ß√£o aqui - ser√° feita no loss)
        x = self.fc3(x)
        
        return x

# Instanciar o modelo
input_size = X_train.shape[1]  # 13 features
hidden_size1 = 64  # 64 neur√¥nios na primeira camada oculta
hidden_size2 = 32  # 32 neur√¥nios na segunda camada oculta
num_classes = 3  # 3 tipos de vinho

model = WineClassifier(input_size, hidden_size1, hidden_size2, num_classes)

print(f"\nüß† Modelo criado:")
print(f"   - Arquitetura: {input_size} -> {hidden_size1} -> {hidden_size2} -> {num_classes}")
print(f"   - Par√¢metros trein√°veis: {sum(p.numel() for p in model.parameters())}")

# ==============================================================================
# 4. CONFIGURA√á√ÉO DO TREINAMENTO (Training Setup)
# ==============================================================================

# Fun√ß√£o de perda (Loss Function)
# CrossEntropyLoss: ideal para classifica√ß√£o multiclasse
criterion = nn.CrossEntropyLoss()

# Otimizador (Optimizer)
# Adam: algoritmo de otimiza√ß√£o adaptativo (vers√£o melhorada do SGD)
optimizer = optim.Adam(
    model.parameters(),  # Par√¢metros do modelo a serem otimizados
    lr=0.001  # Learning rate (taxa de aprendizado)
)

# Scheduler: reduz o learning rate ao longo do tempo
# Isso ajuda o modelo a convergir melhor
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',  # Reduz quando a loss para de diminuir
    patience=5,  # Aguarda 5 √©pocas antes de reduzir
    factor=0.5  # Reduz pela metade
)

print(f"\n‚öôÔ∏è  Configura√ß√£o do treinamento:")
print(f"   - Loss Function: CrossEntropyLoss")
print(f"   - Optimizer: Adam (lr=0.001)")
print(f"   - Scheduler: ReduceLROnPlateau")

# ==============================================================================
# 5. TREINAMENTO DO MODELO (Model Training)
# ==============================================================================

def train_model(model, train_loader, criterion, optimizer, num_epochs):
    """
    Fun√ß√£o para treinar o modelo
    """
    # Listas para armazenar m√©tricas
    train_losses = []
    train_accuracies = []
    
    print(f"\nüöÄ Iniciando treinamento por {num_epochs} √©pocas...\n")
    
    for epoch in range(num_epochs):
        # Modo de treinamento (ativa dropout e outras camadas espec√≠ficas)
        model.train()
        
        running_loss = 0.0  # Acumula a loss da √©poca
        correct = 0  # Conta predi√ß√µes corretas
        total = 0  # Conta total de amostras
        
        # Iterar sobre os batches
        for batch_idx, (data, targets) in enumerate(train_loader):
            # Forward pass: calcular predi√ß√µes
            outputs = model(data)
            
            # Calcular loss (erro entre predi√ß√£o e valor real)
            loss = criterion(outputs, targets)
            
            # Backward pass: calcular gradientes
            optimizer.zero_grad()  # Zerar gradientes anteriores
            loss.backward()  # Calcular novos gradientes (backpropagation)
            
            # Atualizar pesos do modelo
            optimizer.step()
            
            # Estat√≠sticas
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)  # Pegar classe com maior probabilidade
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
        
        # Calcular m√©tricas da √©poca
        epoch_loss = running_loss / len(train_loader)
        epoch_accuracy = 100 * correct / total
        
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_accuracy)
        
        # Atualizar learning rate
        scheduler.step(epoch_loss)
        
        # Imprimir progresso a cada 10 √©pocas
        if (epoch + 1) % 10 == 0:
            print(f"√âpoca [{epoch+1}/{num_epochs}] | "
                  f"Loss: {epoch_loss:.4f} | "
                  f"Acur√°cia: {epoch_accuracy:.2f}%")
    
    print(f"\n‚úì Treinamento conclu√≠do!")
    return train_losses, train_accuracies

# Treinar o modelo por 100 √©pocas
num_epochs = 100
train_losses, train_accuracies = train_model(
    model, train_loader, criterion, optimizer, num_epochs
)

# ==============================================================================
# 6. AVALIA√á√ÉO DO MODELO (Model Evaluation)
# ==============================================================================

def evaluate_model(model, test_loader):
    """
    Avaliar o modelo no conjunto de teste
    """
    # Modo de avalia√ß√£o (desativa dropout)
    model.eval()
    
    correct = 0
    total = 0
    all_predictions = []
    all_targets = []
    
    # Desabilitar c√°lculo de gradientes (economiza mem√≥ria)
    with torch.no_grad():
        for data, targets in test_loader:
            # Forward pass
            outputs = model(data)
            
            # Obter predi√ß√µes
            _, predicted = torch.max(outputs.data, 1)
            
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
            
            all_predictions.extend(predicted.numpy())
            all_targets.extend(targets.numpy())
    
    accuracy = 100 * correct / total
    return accuracy, all_predictions, all_targets

print(f"\nüìà Avaliando modelo no conjunto de teste...")
test_accuracy, predictions, true_labels = evaluate_model(model, test_loader)

print(f"\n{'='*60}")
print(f"RESULTADOS FINAIS")
print(f"{'='*60}")
print(f"Acur√°cia no conjunto de TESTE: {test_accuracy:.2f}%")
print(f"{'='*60}")

# ==============================================================================
# 7. VISUALIZA√á√ÉO DOS RESULTADOS (Results Visualization)
# ==============================================================================

# Criar gr√°fico de loss e acur√°cia
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# Gr√°fico 1: Loss ao longo das √©pocas
ax1.plot(train_losses, label='Training Loss', color='red', linewidth=2)
ax1.set_xlabel('√âpoca', fontsize=12)
ax1.set_ylabel('Loss', fontsize=12)
ax1.set_title('Loss durante o Treinamento', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Gr√°fico 2: Acur√°cia ao longo das √©pocas
ax2.plot(train_accuracies, label='Training Accuracy', color='green', linewidth=2)
ax2.set_xlabel('√âpoca', fontsize=12)
ax2.set_ylabel('Acur√°cia (%)', fontsize=12)
ax2.set_title('Acur√°cia durante o Treinamento', fontsize=14, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
print(f"\n‚úì Gr√°ficos salvos em 'training_results.png'")

# ==============================================================================
# 8. FAZER PREDI√á√ïES (Making Predictions)
# ==============================================================================

def predict_wine_type(model, features, scaler, wine_names):
    """
    Fazer predi√ß√£o para uma nova amostra de vinho
    """
    # Modo de avalia√ß√£o
    model.eval()
    
    # Normalizar features
    features_scaled = scaler.transform([features])
    
    # Converter para tensor
    features_tensor = torch.FloatTensor(features_scaled)
    
    # Fazer predi√ß√£o
    with torch.no_grad():
        output = model(features_tensor)
        
        # Aplicar softmax para obter probabilidades
        probabilities = torch.softmax(output, dim=1)
        
        # Obter classe predita
        _, predicted_class = torch.max(output, 1)
    
    return predicted_class.item(), probabilities[0].numpy()

# Exemplo de predi√ß√£o com uma nova amostra
print(f"\nüç∑ Exemplo de Predi√ß√£o:")
print(f"{'='*60}")

# Usar a primeira amostra do conjunto de teste
sample_features = X_test[0]
true_class = y_test[0]

predicted_class, probabilities = predict_wine_type(
    model, sample_features, scaler, wine_data.target_names
)

print(f"Amostra: {sample_features[:3]}... (primeiras 3 features)")
print(f"\nClasse Real: {wine_data.target_names[true_class]}")
print(f"Classe Predita: {wine_data.target_names[predicted_class]}")
print(f"\nProbabilidades:")
for i, prob in enumerate(probabilities):
    print(f"  - {wine_data.target_names[i]}: {prob*100:.2f}%")

# ==============================================================================
# 9. SALVAR O MODELO (Save Model)
# ==============================================================================

# Salvar modelo treinado
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'input_size': input_size,
    'hidden_size1': hidden_size1,
    'hidden_size2': hidden_size2,
    'num_classes': num_classes
}, 'wine_classifier_model.pth')

print(f"\n‚úì Modelo salvo em 'wine_classifier_model.pth'")

# ==============================================================================
# 10. CARREGAR MODELO (Load Model) - Exemplo
# ==============================================================================

def load_model(filepath):
    """
    Carregar modelo salvo
    """
    checkpoint = torch.load(filepath)
    
    # Recriar modelo com mesma arquitetura
    loaded_model = WineClassifier(
        checkpoint['input_size'],
        checkpoint['hidden_size1'],
        checkpoint['hidden_size2'],
        checkpoint['num_classes']
    )
    
    # Carregar pesos
    loaded_model.load_state_dict(checkpoint['model_state_dict'])
    loaded_model.eval()
    
    return loaded_model

print(f"\n‚úì Para carregar o modelo: loaded_model = load_model('wine_classifier_model.pth')")

print(f"\n{'='*60}")
print(f"PROJETO CONCLU√çDO COM SUCESSO! üéâ")
print(f"{'='*60}\n")